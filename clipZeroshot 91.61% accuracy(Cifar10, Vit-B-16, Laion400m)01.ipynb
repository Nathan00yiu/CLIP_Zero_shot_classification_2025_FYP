{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3d137c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "#!pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "808e51aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XPU available : True\n",
      "GPU 名稱    : Intel(R) Arc(TM) B580 Graphics\n",
      "--------------------------------------------------\n",
      "IPEX + Intel GPU inference 成功\n",
      "輸出 shape : torch.Size([8, 1000])\n",
      "使用裝置   : xpu:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "print(\"XPU available :\", torch.xpu.is_available())\n",
    "if torch.xpu.is_available():\n",
    "    print(\"GPU 名稱    :\", torch.xpu.get_device_name(0))\n",
    "    device = torch.device(\"xpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 建立測試模型\n",
    "model = torch.nn.Linear(512, 1000).to(device)\n",
    "model.eval()\n",
    "\n",
    "# 產生假資料\n",
    "data = torch.randn(8, 512).to(device)\n",
    "\n",
    "# 2025 年最新 IPEX 寫法（只回傳一個值！）\n",
    "model = ipex.optimize(model, dtype=torch.float32)\n",
    "\n",
    "# 真正跑一次\n",
    "with torch.no_grad():\n",
    "    output = model(data)\n",
    "\n",
    "print(\"IPEX + Intel GPU inference 成功\")\n",
    "print(\"輸出 shape :\", output.shape)\n",
    "print(\"使用裝置   :\", output.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aef39fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Intel GPU (XPU)\n",
      "Model optimized with IPEX for Intel GPU\n",
      "Loaded: ViT-B-16 (DataComp-1B)\n",
      "Device: xpu:0\n"
     ]
    }
   ],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex  # Import IPEX for Intel GPU support\n",
    "\n",
    "# Device detection: Prioritize Intel GPU (XPU) over CPU\n",
    "if torch.xpu.is_available():\n",
    "    device = torch.device(\"xpu\")\n",
    "    print(\"Using Intel GPU (XPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Fallback to NVIDIA if available\n",
    "    print(\"Using NVIDIA GPU (CUDA)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Load OpenCLIP model and preprocess (unchanged)\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name=\"ViT-B-16\",\n",
    "    pretrained=\"laion400m_e31\"\n",
    ")\n",
    "\n",
    "# Get tokenizer (unchanged)\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Optional: Optimize with IPEX for better performance on Intel GPU\n",
    "if device.type == \"xpu\":\n",
    "    model = ipex.optimize(model, dtype=torch.bfloat16)  # Use bfloat16 for speed; fallback to float32 if needed\n",
    "    print(\"Model optimized with IPEX for Intel GPU\")\n",
    "\n",
    "print(\"Loaded:\", \"ViT-B-16 (DataComp-1B)\")\n",
    "print(\"Device:\", next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "526f24c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 9 templates.\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: CIFAR-10 classes + smart prompt ensemble (3 templates = perfect speed/accuracy)\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_set = CIFAR10(root='./data', train=False, download=True, transform=preprocess)\n",
    "test_loader = DataLoader(test_set, batch_size=128)\n",
    "\n",
    "cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "templates = [\n",
    "    \"a photo of a {}.\",\n",
    "    \"a blurry photo of a {}.\",\n",
    "    \"a photo of a small {}.\",\n",
    "    \"a close-up photo of a {}.\",\n",
    "    \"a cropped photo of a {}.\",\n",
    "    \"a bright photo of a {}.\",\n",
    "    \"a low-resolution photo of a {}.\",\n",
    "    \"a clean photo of a {}.\",\n",
    "    \"a pixelated photo of a {}.\",\n",
    "]\n",
    "\n",
    "print(f\"Using {len(templates)} templates.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49435f3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type BFloat16 but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m tokens = tokenizer(texts).to(device)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     emb = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     emb = emb / emb.norm(dim=-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     13\u001b[39m text_features.append(emb.mean(\u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\clip-intel\\Lib\\site-packages\\open_clip\\model.py:336\u001b[39m, in \u001b[36mCLIP.encode_text\u001b[39m\u001b[34m(self, text, normalize)\u001b[39m\n\u001b[32m    333\u001b[39m x = \u001b[38;5;28mself\u001b[39m.token_embedding(text).to(cast_dtype)  \u001b[38;5;66;03m# [batch_size, n_ctx, d_model]\u001b[39;00m\n\u001b[32m    335\u001b[39m x = x + \u001b[38;5;28mself\u001b[39m.positional_embedding.to(cast_dtype)\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_final(x)  \u001b[38;5;66;03m# [batch_size, n_ctx, transformer.width]\u001b[39;00m\n\u001b[32m    338\u001b[39m x = text_global_pool(x, text, \u001b[38;5;28mself\u001b[39m.text_pool_type, eos_token_id=\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtext_eos_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\clip-intel\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\clip-intel\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\clip-intel\\Lib\\site-packages\\open_clip\\transformer.py:572\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, attn_mask)\u001b[39m\n\u001b[32m    570\u001b[39m         x = checkpoint(r, x, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, attn_mask, use_reentrant=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    571\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m572\u001b[39m         x = \u001b[43mr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_first:\n\u001b[32m    575\u001b[39m     x = x.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)    \u001b[38;5;66;03m# LND -> NLD\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\clip-intel\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\clip-intel\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\clip-intel\\Lib\\site-packages\\open_clip\\transformer.py:298\u001b[39m, in \u001b[36mResidualAttentionBlock.forward\u001b[39m\u001b[34m(self, q_x, k_x, v_x, attn_mask)\u001b[39m\n\u001b[32m    296\u001b[39m k_x = \u001b[38;5;28mself\u001b[39m.ln_1_kv(k_x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mln_1_kv\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m k_x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    297\u001b[39m v_x = \u001b[38;5;28mself\u001b[39m.ln_1_kv(v_x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mln_1_kv\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m v_x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m x = q_x + \u001b[38;5;28mself\u001b[39m.ls_1(\u001b[38;5;28mself\u001b[39m.attention(q_x=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_x\u001b[49m\u001b[43m)\u001b[49m, k_x=k_x, v_x=v_x, attn_mask=attn_mask))\n\u001b[32m    299\u001b[39m x = x + \u001b[38;5;28mself\u001b[39m.ls_2(\u001b[38;5;28mself\u001b[39m.mlp(\u001b[38;5;28mself\u001b[39m.ln_2(x)))\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\clip-intel\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\clip-intel\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\clip-intel\\Lib\\site-packages\\open_clip\\transformer.py:28\u001b[39m, in \u001b[36mLayerNorm.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m     27\u001b[39m     orig_type = x.dtype\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     x = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x.to(orig_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\miniconda3\\envs\\clip-intel\\Lib\\site-packages\\torch\\nn\\functional.py:2905\u001b[39m, in \u001b[36mlayer_norm\u001b[39m\u001b[34m(input, normalized_shape, weight, bias, eps)\u001b[39m\n\u001b[32m   2895\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[32m   2896\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m   2897\u001b[39m         layer_norm,\n\u001b[32m   2898\u001b[39m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2903\u001b[39m         eps=eps,\n\u001b[32m   2904\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2905\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2906\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\n\u001b[32m   2907\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: expected scalar type BFloat16 but found Float"
     ]
    }
   ],
   "source": [
    "# BUILD TEXT FEATURES (correct tokenizer)\n",
    "text_features = []\n",
    "\n",
    "for classname in cifar10_classes:\n",
    "    texts = [t.format(classname) for t in templates]\n",
    "    tokens = tokenizer(texts).to(device)\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = model.encode_text(tokens)\n",
    "        emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_features.append(emb.mean(0))\n",
    "\n",
    "text_features = torch.stack(text_features)\n",
    "text_features = text_features / text_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ebf998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ZERO-SHOT\n",
    "all_predictions = []\n",
    "all_scores = []\n",
    "true_labels = np.array(test_set.targets)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, _ in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "\n",
    "        img_feat = model.encode_image(images)\n",
    "        img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        sim = (100 * img_feat @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "        all_scores.append(sim.cpu())\n",
    "        all_predictions.extend(sim.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "predictions = np.array(all_predictions)\n",
    "full_scores = torch.cat(all_scores).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15d97f54",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Convert everything to numpy once\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m y_true = \u001b[43mtrue_labels\u001b[49m\n\u001b[32m     13\u001b[39m y_pred = predictions\n\u001b[32m     14\u001b[39m y_scores = full_scores  \u001b[38;5;66;03m# shape (10000, 10)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'true_labels' is not defined"
     ]
    }
   ],
   "source": [
    "# CELL 6 – ALL METRICS YOU WILL EVER NEED (research-grade)\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, top_k_accuracy_score,\n",
    "    balanced_accuracy_score, matthews_corrcoef, precision_recall_fscore_support\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert everything to numpy once\n",
    "y_true = true_labels\n",
    "y_pred = predictions\n",
    "y_scores = full_scores  # shape (10000, 10)\n",
    "\n",
    "# 1. Top-k accuracies\n",
    "top1 = np.mean(y_pred == y_true)\n",
    "top3 = top_k_accuracy_score(y_true, y_scores, k=3)\n",
    "top5 = top_k_accuracy_score(y_true, y_scores, k=5)\n",
    "\n",
    "# 2. All classification metrics\n",
    "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "# 3. Specificity (macro-averaged)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "specificity_per_class = []\n",
    "for i in range(10):\n",
    "    tn = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - cm[i, i])\n",
    "    fp = cm[:, i].sum() - cm[i, i]\n",
    "    specificity_per_class.append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
    "specificity_macro = np.mean(specificity_per_class)\n",
    "\n",
    "# 4. Per-class accuracy\n",
    "per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "# PRINT EVERYTHING BEAUTIFULLY\n",
    "print(\"ZERO-SHOT CIFAR-10 – COMPLETE RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Top-1 Accuracy       : {top1:.4f}  →  {top1*100:05.2f}%\")\n",
    "print(f\"Top-3 Accuracy       : {top3:.4f}  →  {top3*100:05.2f}%\")\n",
    "print(f\"Top-5 Accuracy       : {top5:.4f}  →  {top5*100:05.2f}%\")\n",
    "print(f\"Balanced Accuracy    : {balanced_acc:.4f}\")\n",
    "print(f\"Matthews Corr Coef   : {mcc:.4f}\")\n",
    "print(f\"Macro F1-score       : {f1_macro:.4f}\")\n",
    "print(f\"Weighted F1-score    : {f1_weighted:.4f}\")\n",
    "print(f\"Macro Specificity    : {specificity_macro:.4f}\")\n",
    "\n",
    "# Per-class table (sorted)\n",
    "df_per_class = pd.DataFrame({\n",
    "    'Class': cifar10_classes,\n",
    "    'Accuracy': per_class_acc,\n",
    "    'Specificity': specificity_per_class\n",
    "}).sort_values('Accuracy')\n",
    "\n",
    "print(\"\\nPer-class Accuracy (worst → best)\")\n",
    "print(df_per_class.round(4).to_string(index=False))\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(11, 9))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=cifar10_classes, yticklabels=cifar10_classes,\n",
    "            cbar=False, linewidths=0.5, linecolor='gray')\n",
    "plt.title('Zero-Shot CLIP – CIFAR-10 Confusion Matrix\\n(ViT-B-16 + openai)', fontsize=18)\n",
    "plt.xlabel('Predicted', fontsize=14)\n",
    "plt.ylabel('True', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip-intel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
